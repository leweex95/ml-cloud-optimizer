{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7f7a88",
   "metadata": {},
   "source": [
    "# Cloud Workload Optimization - Exploratory Data Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates comprehensive exploratory data analysis (EDA) of synthetic cloud workload telemetry data, including:\n",
    "- Data generation and characteristics\n",
    "- Statistical distributions and correlations\n",
    "- Temporal patterns and anomalies\n",
    "- Imbalanced event analysis\n",
    "- Missing value patterns\n",
    "\n",
    "**Dataset**: 100K+ synthetic cloud telemetry records  \n",
    "**Time Period**: 90 days  \n",
    "**Services**: 20 unique services across 5 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4b84c",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed62fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, '/workspace/src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644c0056",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b20b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_cloud_optimizer.data.generator import CloudWorkloadGenerator, create_sample_dataset\n",
    "\n",
    "# Generate dataset\n",
    "logger.info(\"Generating synthetic cloud workload data...\")\n",
    "df = create_sample_dataset(output_path='../data/raw/cloud_workloads.csv')\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "print(f\"\\nDataset generated successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"Duration: {(df['timestamp'].max() - df['timestamp'].min()).days} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c84fb8",
   "metadata": {},
   "source": [
    "## 3. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c3da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08052808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64146612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print(\"\\nMissing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing_Count': missing.values,\n",
    "    'Percentage': missing_pct.values\n",
    "})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Unique values\n",
    "print(f\"\\nUnique Services: {df['service_id'].nunique()}\")\n",
    "print(f\"Unique Clusters: {df['cluster_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4fa3c",
   "metadata": {},
   "source": [
    "## 4. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2804564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource utilization distributions\n",
    "metrics = ['cpu_utilization', 'memory_utilization', 'network_utilization', 'cost']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(df[metric], bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Distribution of {metric}')\n",
    "    \n",
    "    # Add statistics\n",
    "    mean = df[metric].mean()\n",
    "    median = df[metric].median()\n",
    "    ax.axvline(mean, color='r', linestyle='--', label=f'Mean: {mean:.3f}')\n",
    "    ax.axvline(median, color='g', linestyle='--', label=f'Median: {median:.3f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Distribution plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c33202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    ax.boxplot(df[metric], vert=True)\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'Box plot: {metric}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/boxplots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Box plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed619ad",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f888bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "correlation_matrix = df[metrics].corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Correlation Matrix - Resource Metrics')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Correlation plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c7d22",
   "metadata": {},
   "source": [
    "## 6. Temporal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9dfbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temporal features\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "\n",
    "# Hourly patterns\n",
    "hourly_stats = df.groupby('hour')[metrics].mean()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.plot(hourly_stats.index, hourly_stats[metric], marker='o', linewidth=2)\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'Hourly Pattern: {metric}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xticks(range(0, 24, 2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/hourly_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Hourly pattern plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3157c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily patterns (weekday vs weekend)\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_stats = df.groupby('day_of_week')[metrics].mean()\n",
    "daily_stats.index = [day_names[i] for i in daily_stats.index]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    colors = ['#1f77b4']*5 + ['#ff7f0e']*2  # Blue for weekdays, orange for weekends\n",
    "    ax.bar(daily_stats.index, daily_stats[metric], color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'Daily Pattern: {metric}')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/daily_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Daily pattern plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af30013",
   "metadata": {},
   "source": [
    "## 7. Imbalanced Event Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a04b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling events analysis\n",
    "print(\"=\"*80)\n",
    "print(\"SCALING EVENTS ANALYSIS (Imbalanced Events)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "scaling_counts = df['scale_event'].value_counts()\n",
    "print(f\"\\nScaling Event Distribution:\")\n",
    "print(f\"No Scaling (0): {scaling_counts.get(0, 0)} ({scaling_counts.get(0, 0)/len(df)*100:.2f}%)\")\n",
    "print(f\"Scaling (1): {scaling_counts.get(1, 0)} ({scaling_counts.get(1, 0)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Compare metrics when scaling occurs vs not\n",
    "comparison = df.groupby('scale_event')[metrics].agg(['mean', 'std', 'min', 'max'])\n",
    "print(\"\\nMetrics Comparison (Scaling vs No Scaling):\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3068247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaling events\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[f'{metric} - Scaling vs No Scaling' for metric in metrics],\n",
    "    vertical_spacing=0.12,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e']  # Blue: no scaling, Orange: scaling\n",
    "scale_groups = [0, 1]\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    row = idx // 2 + 1\n",
    "    col = idx % 2 + 1\n",
    "    \n",
    "    for scale_group, color in zip(scale_groups, colors):\n",
    "        data = df[df['scale_event'] == scale_group][metric]\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=data,\n",
    "                name=f\"Scale={scale_group}\",\n",
    "                marker_color=color,\n",
    "                boxmean='sd'\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Resource Metrics Distribution: Scaling vs No Scaling\", showlegend=True)\n",
    "fig.write_html('../data/processed/scaling_comparison.html')\n",
    "fig.show()\n",
    "\n",
    "print(\"✓ Scaling comparison plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal distribution of scaling events\n",
    "scaling_by_hour = df[df['scale_event'] == 1].groupby('hour').size()\n",
    "scaling_by_day = df[df['scale_event'] == 1].groupby('day_of_week').size()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# By hour\n",
    "axes[0].bar(scaling_by_hour.index, scaling_by_hour.values, color='#ff7f0e', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Hour of Day')\n",
    "axes[0].set_ylabel('Number of Scaling Events')\n",
    "axes[0].set_title('Scaling Events by Hour of Day')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# By day\n",
    "day_labels = [day_names[i] for i in scaling_by_day.index]\n",
    "colors = ['#1f77b4']*5 + ['#ff7f0e']*2\n",
    "axes[1].bar(day_labels, scaling_by_day.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Number of Scaling Events')\n",
    "axes[1].set_title('Scaling Events by Day of Week')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/scaling_temporal.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Temporal scaling plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe00bb",
   "metadata": {},
   "source": [
    "## 8. Service & Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3800d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top resource consumers\n",
    "service_stats = df.groupby('service_id').agg({\n",
    "    'cpu_utilization': 'mean',\n",
    "    'memory_utilization': 'mean',\n",
    "    'cost': 'sum',\n",
    "    'scale_event': 'sum'\n",
    "}).round(4)\n",
    "service_stats.columns = ['Avg CPU', 'Avg Memory', 'Total Cost', 'Scaling Events']\n",
    "service_stats = service_stats.sort_values('Total Cost', ascending=False)\n",
    "\n",
    "print(\"Top 10 Resource Consumers (by Cost):\")\n",
    "print(service_stats.head(10))\n",
    "\n",
    "# Cluster analysis\n",
    "cluster_stats = df.groupby('cluster_id').agg({\n",
    "    'cpu_utilization': 'mean',\n",
    "    'memory_utilization': 'mean',\n",
    "    'network_utilization': 'mean',\n",
    "    'cost': 'sum',\n",
    "    'scale_event': 'sum'\n",
    "}).round(4)\n",
    "cluster_stats.columns = ['Avg CPU', 'Avg Memory', 'Avg Network', 'Total Cost', 'Scaling Events']\n",
    "\n",
    "print(\"\\nCluster-level Statistics:\")\n",
    "print(cluster_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab40ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top services\n",
    "top_10_services = service_stats.head(10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cost\n",
    "axes[0].barh(range(len(top_10_services)), top_10_services['Total Cost'].values, color='#2ca02c', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_yticks(range(len(top_10_services)))\n",
    "axes[0].set_yticklabels(top_10_services.index)\n",
    "axes[0].set_xlabel('Total Cost (USD)')\n",
    "axes[0].set_title('Top 10 Services by Cost')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Scaling events\n",
    "axes[1].barh(range(len(top_10_services)), top_10_services['Scaling Events'].values, color='#ff7f0e', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_yticks(range(len(top_10_services)))\n",
    "axes[1].set_yticklabels(top_10_services.index)\n",
    "axes[1].set_xlabel('Number of Scaling Events')\n",
    "axes[1].set_title('Top 10 Services by Scaling Events')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/top_services.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Top services plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25ea9b",
   "metadata": {},
   "source": [
    "## 9. Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values\n",
    "print(\"Missing Value Analysis:\")\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum().values,\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2).values\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    print(missing_summary)\n",
    "    \n",
    "    # Visualize missing data\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(x=missing_summary['Column'], y=missing_summary['Missing_Percentage'],\n",
    "               marker_color='#d62728')\n",
    "    ])\n",
    "    fig.update_layout(\n",
    "        title='Missing Data Percentage by Column',\n",
    "        xaxis_title='Column',\n",
    "        yaxis_title='Missing Percentage (%)',\n",
    "        height=400\n",
    "    )\n",
    "    fig.write_html('../data/processed/missing_data.html')\n",
    "    fig.show()\n",
    "    print(\"✓ Missing data plot saved\")\n",
    "else:\n",
    "    print(\"No missing values after initial cleanup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190a997a",
   "metadata": {},
   "source": [
    "## 10. Key Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f82262",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS FROM EDA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "insights = f\"\"\"\n",
    "1. DATASET CHARACTERISTICS:\n",
    "   - Total Records: {len(df):,}\n",
    "   - Date Range: {df['timestamp'].min().date()} to {df['timestamp'].max().date()}\n",
    "   - Services: {df['service_id'].nunique()} unique services\n",
    "   - Clusters: {df['cluster_id'].nunique()} unique clusters\n",
    "\n",
    "2. RESOURCE UTILIZATION:\n",
    "   - CPU: Mean={df['cpu_utilization'].mean():.4f}, Std={df['cpu_utilization'].std():.4f}\n",
    "   - Memory: Mean={df['memory_utilization'].mean():.4f}, Std={df['memory_utilization'].std():.4f}\n",
    "   - Network: Mean={df['network_utilization'].mean():.4f}, Std={df['network_utilization'].std():.4f}\n",
    "\n",
    "3. COST ANALYSIS:\n",
    "   - Total Cost: ${df['cost'].sum():.2f}\n",
    "   - Average Cost per Record: ${df['cost'].mean():.6f}\n",
    "   - Cost per Observation: ${df.groupby('timestamp')['cost'].sum().mean():.4f}\n",
    "\n",
    "4. IMBALANCED EVENTS (Peak Loads):\n",
    "   - Scaling Events: {df['scale_event'].sum()} out of {len(df):,} ({df['scale_event'].sum()/len(df)*100:.2f}%)\n",
    "   - Imbalance Ratio: 1:{int(len(df)/(df['scale_event'].sum()+1))}\n",
    "   - This is a realistic imbalanced classification problem\n",
    "\n",
    "5. TEMPORAL PATTERNS:\n",
    "   - Peak Hours: 8-18 (business hours)\n",
    "   - Weekday vs Weekend: {df[df['day_of_week']<5]['cpu_utilization'].mean():.4f} vs {df[df['day_of_week']>=5]['cpu_utilization'].mean():.4f}\n",
    "   - Clear diurnal pattern observed\n",
    "\n",
    "6. CORRELATIONS:\n",
    "   - CPU-Memory Correlation: {correlation_matrix.loc['cpu_utilization', 'memory_utilization']:.4f} (Strong positive)\n",
    "   - CPU-Network Correlation: {correlation_matrix.loc['cpu_utilization', 'network_utilization']:.4f}\n",
    "   - Memory-Cost Correlation: {correlation_matrix.loc['memory_utilization', 'cost']:.4f}\n",
    "\n",
    "7. TOP RESOURCE CONSUMERS:\n",
    "   - Top 5 services account for {(service_stats['Total Cost'].head(5).sum() / service_stats['Total Cost'].sum() * 100):.1f}% of total cost\n",
    "   - Service concentration suggests opportunity for targeted optimization\n",
    "\n",
    "8. DATA QUALITY:\n",
    "   - Missing Values: {df.isnull().sum().sum()} total missing values\n",
    "   - Duplicates: {df.duplicated().sum()} duplicate rows\n",
    "   - Ready for feature engineering and modeling\n",
    "\n",
    "9. RECOMMENDATIONS FOR MODELING:\n",
    "   - Use time-series aware cross-validation (temporal data)\n",
    "   - Apply SMOTE or class weighting for imbalanced event prediction\n",
    "   - Include lag features and rolling statistics\n",
    "   - Consider separate models for peak vs off-peak periods\n",
    "   - Normalize features (wide range: [0, 1] for CPU/Memory, [0, 100] for Network)\n",
    "\"\"\"\n",
    "\n",
    "print(insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e512b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "df_summary = df.copy()\n",
    "df_summary.to_csv('../data/processed/eda_summary.csv', index=False)\n",
    "\n",
    "# Save statistics\n",
    "stats_dict = {\n",
    "    'Total Records': len(df),\n",
    "    'Unique Services': df['service_id'].nunique(),\n",
    "    'Unique Clusters': df['cluster_id'].nunique(),\n",
    "    'Date Range (Days)': (df['timestamp'].max() - df['timestamp'].min()).days,\n",
    "    'Scaling Events': int(df['scale_event'].sum()),\n",
    "    'Scaling Event Rate (%)': round(df['scale_event'].sum()/len(df)*100, 2),\n",
    "    'Avg CPU': round(df['cpu_utilization'].mean(), 4),\n",
    "    'Avg Memory': round(df['memory_utilization'].mean(), 4),\n",
    "    'Total Cost': round(df['cost'].sum(), 2)\n",
    "}\n",
    "\n",
    "stats_df = pd.DataFrame([stats_dict]).T\n",
    "stats_df.columns = ['Value']\n",
    "stats_df.to_csv('../data/processed/eda_statistics.csv')\n",
    "\n",
    "print(\"\\n✓ EDA complete! Summary statistics saved.\")\n",
    "print(f\"  - eda_summary.csv: Full dataset with temporal features\")\n",
    "print(f\"  - eda_statistics.csv: Summary statistics\")\n",
    "print(f\"  - Visualizations saved in ../data/processed/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
